{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227c2716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "matplotlib.rcParams[\"figure.figsize\"] = (20, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efbcb96",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c82457b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from csv to dataframe\n",
    "df = pd.read_csv(\"Bengaluru_House_Data.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185dae10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size of the dataset\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7aaed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Groupby area_type and then apply count aggregate \n",
    "df.groupby('area_type')['area_type'].agg('count')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f5d2b5",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440d2e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the columns which may not be useful for price detection\n",
    "df2 = df.drop(['area_type', 'society', 'balcony', 'availability'], axis='columns')\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091b7600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns total number of NA for that column\n",
    "df2.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442b3128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop NA\n",
    "df3 = df2.dropna()\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929b8693",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3[\"size\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433701f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3[\"bhk\"] = df3[\"size\"].apply(lambda x: int(x.split(\" \")[0]))\n",
    "df3[df3.bhk>20]\n",
    "df3.total_sqft.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57fce5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_float(x):\n",
    "    try:\n",
    "        float(x)\n",
    "    except:\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a079d735",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3[~df3[\"total_sqft\"].apply(is_float)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff937ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_sqft_to_num(x):\n",
    "    tokens = x.split(\"-\")\n",
    "    if len(tokens) == 2:\n",
    "        return (float(tokens[0])+float(tokens[1]))/2\n",
    "    try:\n",
    "        return float(x)\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2c843a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = df3.copy()\n",
    "df4['total_sqft'] = df4['total_sqft'].apply(convert_sqft_to_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48e8de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f581e79c",
   "metadata": {},
   "source": [
    "### Feature Engineering & Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2e9041",
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = df4.copy()\n",
    "# Adding a new feature with the help of availabile columns is called feature engineering\n",
    "df5[\"price_per_sqft\"] = df5[\"price\"]*100000/df5[\"total_sqft\"]\n",
    "df5.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a29ff19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# High dimensionality problem - There are so many locations available, \n",
    "# we will call locations as \"other\" if they have are less than 10\n",
    "len(df5[\"location\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56491f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Extra spaces in location\n",
    "df5[\"location\"] = df5[\"location\"].apply(lambda x: x.strip())\n",
    "location_stats = df5.groupby(\"location\")[\"location\"].agg(\"count\").sort_values(ascending=False)\n",
    "location_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9f55c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(location_stats[location_stats<10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45896d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "location_stats_less_than_10 = location_stats[location_stats<10]\n",
    "location_stats_less_than_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cec4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensionality reduction\n",
    "df5[\"location\"] = df5[\"location\"].apply(lambda x: \"other\" if x in location_stats_less_than_10 else x)\n",
    "len(df5[\"location\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e16f651",
   "metadata": {},
   "source": [
    "### Outlier Detection & Outlier Remover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803e96f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use standard deviation or domain knowledge to remove outliers.\n",
    "# For eg, in domain knowledge, it is not possible to have 40 bedrooms per 2000 sq ft.\n",
    "# We can assume that, 300 sq.ft is equivalent to 1 bedroom. Consider this as threshold.\n",
    "# So here, we can divide the total_sqft by threshold sqft to remove the outliers\n",
    "df6 = df5[~(df5[\"total_sqft\"]/df5.bhk<300)]\n",
    "df6.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91189096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the statistics for this column, remove the extreme cases.\n",
    "# Around 68% data lies between mean and one standard deviation\n",
    "df6[\"price_per_sqft\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7eca7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to find mean standard deviation per location and filter out datapoints which are beyond one standard deviation.\n",
    "# Remove price per square outliers\n",
    "def remove_pps_outliers(df):\n",
    "    df_out = pd.DataFrame()\n",
    "    for key, subdf in df.groupby('location'):\n",
    "        m = np.mean(subdf.price_per_sqft)\n",
    "        st = np.std(subdf.price_per_sqft)\n",
    "        reduced_df = subdf[(subdf.price_per_sqft>(m-st)) & (subdf.price_per_sqft<=(m+st))]\n",
    "        df_out = pd.concat([df_out, reduced_df], ignore_index=True)\n",
    "    return df_out\n",
    "df7 = remove_pps_outliers(df6)\n",
    "df7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab71c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we want to check property price for 3 bedrooms apartments are \n",
    "# more than of 2 bedrooms aprartments or not for same square foot\n",
    "# Visualization which tells how many such cases are present can be done using scatter plot\n",
    "def plot_scatter_chart(df, location):\n",
    "    bhk2 = df[(df.location==location) & (df.bhk==2)]\n",
    "    bhk3 = df[(df.location==location) & (df.bhk==3)]\n",
    "    matplotlib.rcParams[\"figure.figsize\"] = (15, 10)\n",
    "    plt.scatter(bhk2.total_sqft, bhk2.price_per_sqft, color='blue', label='2 BHK', s=50)\n",
    "    plt.scatter(bhk3.total_sqft, bhk3.price_per_sqft, marker='+', color='green', label='3 BHK', s=50)\n",
    "    plt.xlabel(\"Total square feet area\")\n",
    "    plt.ylabel(\"Price\")\n",
    "    plt.title(location)\n",
    "    plt.legend()\n",
    "\n",
    "plot_scatter_chart(df7, \"Rajaji Nagar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcbc5aa",
   "metadata": {},
   "source": [
    "###### Remove properties with 3 bedroom apartments price is less than 2 bedroom apartments. We can remove those 2 BHK apartments whose price_per_sqft is less than mean price_per_sqft of 1 BHK apartment.\n",
    "For eg, \n",
    "<br>{<br>\n",
    "&emsp;'1': {<br>\n",
    "&emsp;&emsp;'mean': 4000,<br>\n",
    "&emsp;&emsp;'std': 2000,<br>\n",
    "&emsp;&emsp;'count': 34<br>\n",
    "&emsp;},<br>\n",
    "&emsp;'2': {<br>\n",
    "&emsp;&emsp;'mean': 4300,<br>\n",
    "&emsp;&emsp;'std': 2300,<br>\n",
    "&emsp;&emsp;'count': 22<br>\n",
    "&emsp;},<br>\n",
    "}<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf80df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_bhk_outliers(df):\n",
    "    exclude_indices = np.array([])\n",
    "    for location, location_df in df.groupby(\"location\"):\n",
    "        bhk_stats = {}\n",
    "        for bhk, bhk_df in location_df.groupby(\"bhk\"):\n",
    "            bhk_stats[bhk] = {\n",
    "                \"mean\": np.mean(bhk_df.price_per_sqft),\n",
    "                \"std\": np.std(bhk_df.price_per_sqft),\n",
    "                \"count\": bhk_df.shape[0]\n",
    "            }\n",
    "        for bhk, bhk_df in location_df.groupby(\"bhk\"):\n",
    "            stats = bhk_stats.get(bhk-1)\n",
    "            if stats and stats[\"count\"]>5:\n",
    "                exclude_indices = np.append(exclude_indices, bhk_df[bhk_df.price_per_sqft<(stats[\"mean\"])].index.values)\n",
    "    return df.drop(exclude_indices, axis='index')\n",
    "df8 = remove_bhk_outliers(df7)\n",
    "df8.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8228b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, the outliers are removed\n",
    "plot_scatter_chart(df8, \"Rajaji Nagar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecce07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many apartments per square feet area\n",
    "# Histogram\n",
    "plt.hist(df8.price_per_sqft, rwidth=0.8)\n",
    "plt.xlabel(\"Price per square feet\")\n",
    "plt.ylabel(\"Count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4b5aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df8.bath.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a94f7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is there we have any criteria to remove bathroom.\n",
    "# For eg, 2 BHK apartment have maximum 2 bathrooms. If 2 BHK apartments has 6 bathroom then it is weird.\n",
    "df8[df8.bath>10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ffdd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df8.bath, rwidth=0.8)\n",
    "plt.xlabel(\"Number of bathrooms\")\n",
    "plt.ylabel(\"Count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0584516d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's assume, we have criteria that anytime \n",
    "# we have a bathrooms greater than number of bedrooms + 2 then consider it as outlier\n",
    "# Bathroom Outlier = No. of bathrooms > No. of bedrooms + 2\n",
    "df9 = df8[df8.bath<df8.bhk+2]\n",
    "df9.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af4891b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unwanted features\n",
    "# Remove price_per_sqft, as we don't need it in Machine Learning because we used it for outlier detection\n",
    "# Remove size column, as we have bhk column in the dataframe.\n",
    "df10 = df9.drop(['size', 'price_per_sqft'], axis='columns')\n",
    "df10.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d199cc",
   "metadata": {},
   "source": [
    "The output of df10.head() shows the text data for location attribute. But Machine Learning model can't understand the text data, so we have to convert that into the numerical. For that, we will use one-hot encoding method, also called dummies the method of pandas. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937a5152",
   "metadata": {},
   "source": [
    "#### One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1475429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It will creates column for each location and set the value 1 when specific location appeared \n",
    "# and set rest of the locations as 0.\n",
    "dummies = pd.get_dummies(df10.location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5064437f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append dummies df to new df\n",
    "# To avoid dummy variables trap, you should have one less dummies column.\n",
    "# For eg, let's drop last column which is 'other'\n",
    "df11 = pd.concat([df10, dummies.drop('other', axis='columns')], axis=\"columns\")\n",
    "df11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b663ded7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can drop location column\n",
    "df12 = df11.drop('location', axis=\"columns\")\n",
    "df12.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfe4f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape of df12\n",
    "# df12.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6cc9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the dependent variable price, so x can contains \n",
    "# only independent variables and we can use price in model\n",
    "X = df12.drop(\"price\", axis=\"columns\")\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d6d98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependent variable\n",
    "y = df12.price\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab82cbb",
   "metadata": {},
   "source": [
    "### Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb4d939",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571af73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Linear regression classifier\n",
    "lr_clf = LinearRegression()\n",
    "\n",
    "# Training of the model\n",
    "lr_clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the score of the model (How good our model is.)\n",
    "lr_clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d51e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Shuffle split: Randomize the samples, Each fold have equal distribution\n",
    "cv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=0)\n",
    "\n",
    "# Majority time we're getting more than 80% score using cross_val_score for our example.\n",
    "cross_val_score(LinearRegression(), X, y, cv=cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2ab95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As a data scientist, we've to check which method is giving best score.\n",
    "# There are different algorithms for calculating the score of the model.\n",
    "# For that, we use grid search cv (which runs a model on different regressors and parameters and tells best score).\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Other than Linear Regression, we also want to tries the Lasso and DescisionTree Regressor\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "def find_best_model_using_gridsearchcv(X, y):\n",
    "    algos = {\n",
    "        \"linear_regression\": {\n",
    "            \"model\": LinearRegression(),\n",
    "            \"params\": {\n",
    "                \"normalize\": [True, False]\n",
    "            } # Hyper parameter tuning (GridSearchCV also tells which is best paramter)\n",
    "        },\n",
    "        \"lasso\": {\n",
    "            \"model\": Lasso(),\n",
    "            \"params\": {\n",
    "                \"alpha\": [1, 2],\n",
    "                \"selection\": [\"random\", \"cyclic\"]  \n",
    "            } # Hyper parameter tuning\n",
    "        },\n",
    "        \"decision_tree\": {\n",
    "            \"model\": DecisionTreeRegressor(),\n",
    "            \"params\": {\n",
    "                \"criterion\": [\"mse\", \"friedman_mse\"],\n",
    "                \"splitter\": [\"best\", \"random\"]\n",
    "            } # Hyper parameter tuning\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    scores = []\n",
    "    cv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=0)\n",
    "    \n",
    "    for algo_name, config in algos.items():\n",
    "        gs = GridSearchCV(config[\"model\"], config[\"params\"], cv=cv, return_train_score=False)\n",
    "        gs.fit(X, y)\n",
    "        scores.append({\n",
    "            \"model\": algo_name,\n",
    "            \"best_score\": gs.best_score_,\n",
    "            \"best_params\": gs.best_params_\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(scores, columns=[\"model\", \"best_score\", \"best_params\"])\n",
    "\n",
    "# As a result, the function returns that linear regression model returns the best score\n",
    "find_best_model_using_gridsearchcv(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d61269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will go with the linear regression only to predict the price\n",
    "def predict_price(location, sqft, bath, bhk):\n",
    "    loc_index = np.where(X.columns==location)[0][0]\n",
    "    \n",
    "    x = np.zeros(len(X.columns))\n",
    "    x[0] = sqft\n",
    "    x[1] = bath\n",
    "    x[2] = bhk\n",
    "    \n",
    "    if loc_index >= 0:\n",
    "        x[loc_index] = 1\n",
    "    \n",
    "    return lr_clf.predict([x])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ce39ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test our function for 1st Phase JP Nagar with square foot 1000 contains 2 bathroom with 2 bhk\n",
    "predict_price(\"1st Phase JP Nagar\", 1000, 2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18645e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's predict for high price locations\n",
    "predict_price(\"Indira Nagar\", 1000, 2, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1c41aa",
   "metadata": {},
   "source": [
    "### Export Model to Pickle File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a903d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"banglore_home_prices_model.pickle\", \"wb\") as f:\n",
    "    # Pass your model and file object in the pickle dump function\n",
    "    pickle.dump(lr_clf, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca9e9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other than model, we also need column information\n",
    "import json\n",
    "columns = {\n",
    "    \"data_columns\": [col.lower() for col in X.columns]\n",
    "}\n",
    "with open(\"columns.json\", \"w\") as f:\n",
    "    f.write(json.dumps(columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbbcfbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "8c3d4d9970fc5c09c50d9318a9c7fbf4d0159e4b28833276f5678ee22bded273"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
